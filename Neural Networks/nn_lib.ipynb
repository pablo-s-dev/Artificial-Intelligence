{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "\n",
    "    class Math:\n",
    "        \n",
    "        @classmethod\n",
    "        def _sigmoid(cls, Z: np.ndarray):\n",
    "            return 1/(1+np.exp(-Z)) \n",
    "\n",
    "        @classmethod\n",
    "        def _sigmoid_derivative(cls, Z: np.ndarray):\n",
    "            s = cls._sigmoid(Z)\n",
    "            return s * (1-s) \n",
    "\n",
    "        @classmethod\n",
    "        def _reLU_derivative(cls, Z: np.ndarray):\n",
    "            return (Z > 0).astype(float)\n",
    "        \n",
    "        @classmethod\n",
    "        def _reLU(cls, Z: np.ndarray):\n",
    "            return Z * (Z > 0)\n",
    "\n",
    "        activation_derivative_fun_by_str = {\n",
    "            \"sigmoid\": lambda Z: NN.Math._sigmoid_derivative(Z),\n",
    "            \"reLU\": lambda Z: NN.Math._reLU_derivative(Z)\n",
    "        }\n",
    "\n",
    "        activation_fun_by_str = {\n",
    "            \"sigmoid\": lambda Z: NN.Math._sigmoid(Z),\n",
    "            \"reLU\": lambda Z: NN.Math._reLU(Z)\n",
    "        }\n",
    "\n",
    "    class Layer:\n",
    "    \n",
    "        def __init__(self, neurons_amount: int, activation: Literal[\"sigmoid\", \"reLU\"], is_first: bool=False):\n",
    "\n",
    "            self.activation_fun = NN.Math.activation_fun_by_str[activation]\n",
    "            self.activation_derivative_fun = NN.Math.activation_derivative_fun_by_str[activation]\n",
    "            self.neurons_amount = neurons_amount\n",
    "            self.activation = activation\n",
    "            self.is_first = is_first\n",
    "\n",
    "        def init_params(self):\n",
    "            self.W = np.random.randn(self.input_amount, self.neurons_amount, )\n",
    "            self.B = np.zeros((1, self.neurons_amount))\n",
    "        \n",
    "        def activate(self, X: np.ndarray):\n",
    "\n",
    "            # shape: 1 x neurons_amount (row vector)\n",
    "\n",
    "            self.X = X\n",
    "\n",
    "            self.Z = self.X @ self.W  + self.B\n",
    "\n",
    "            self.activations = self.activation_fun(self.Z)     \n",
    "\n",
    "            return self.activations\n",
    "\n",
    "        def backward(self, DJ_DA: np.array, lr: float):\n",
    "\n",
    "            \"\"\"\n",
    "                @param: DJ_DA How much the activation of this layer affects the error. DJ_DA might be DJ_DX of the layer ahead (if any), since the activations are used as inputs for the layer ahead.\n",
    "                @param: lr Learning rate, the percentage of the gradient that we'll use to determine the step size for updating the parameters. Ex.: 0.1 as lr means we'll update in 10% of the gradient, but in reversed direction.\n",
    "            \"\"\"\n",
    "\n",
    "            DJ_DZ = DJ_DA * self.activation_derivative_fun(self.Z)\n",
    "\n",
    "            DJ_DW = (self.X.transpose(0, 2, 1) @ DJ_DZ )            \n",
    "\n",
    "            DJ_DX = DJ_DZ @ self.W.transpose(1, 0) if self.is_first == False else 0\n",
    "\n",
    "            self.W -=  lr *  np.mean(DJ_DW, axis=0)\n",
    "            self.B -=  lr *  np.mean(DJ_DZ, axis=0)\n",
    "\n",
    "            return DJ_DX\n",
    "        \n",
    "\n",
    "    def __init__(self, layers: list[Layer], input_amount: int):\n",
    "\n",
    "        self.input_amount = input_amount\n",
    "\n",
    "        for i, layer in enumerate(layers):\n",
    "\n",
    "            layer_input_amount = layers[i-1].neurons_amount if i!=0 else self.input_amount\n",
    "\n",
    "            layer.input_amount = layer_input_amount\n",
    "            layer.init_params()\n",
    "            layer.is_first = i == 0\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "    def add_layer(self, layer: Layer):\n",
    "\n",
    "        if len(self.layers) != 0:\n",
    "            layer_input_amount = self.layers[-1].neurons_amount\n",
    "            is_first = False\n",
    "        else:\n",
    "            layer_input_amount = self.input_amount\n",
    "            is_first = True\n",
    "\n",
    "            layer.input_amount = layer_input_amount\n",
    "            layer.init_params()\n",
    "        layer.is_first = is_first\n",
    "\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward_prop(self, input: np.ndarray):\n",
    "\n",
    "        activations: np.ndarray = input\n",
    "\n",
    "        for layer in self.layers:\n",
    "            \n",
    "            activations = layer.activate(activations)\n",
    "\n",
    "        return activations\n",
    "    \n",
    "    def train(self, X, Y, epochs: int = 100, optimizer = '', batch_percentage = 100, glr = 1, ):\n",
    "\n",
    "        y_hat = self.forward_prop(X)\n",
    "\n",
    "        L = len(self.layers)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            DJ_DA = y_hat - Y\n",
    "\n",
    "            for i in reversed(range(L)):\n",
    "\n",
    "                layer = self.layers[i]\n",
    "                DJ_DA = layer.backward(DJ_DA, glr)\n",
    "\n",
    "            y_hat = self.forward_prop(X)\n",
    "\n",
    "            if(epoch % 100 == 0):\n",
    "                print(f\"Cost for epoch {epoch+1}: {self.cost(y_hat, Y)}\")\n",
    "\n",
    "    def cost(self, y_hat, Y):\n",
    "\n",
    "        return (1/2) * np.mean((y_hat - Y)**2)\n",
    "    \n",
    "    def bin_predict(self, X):\n",
    "\n",
    "        return np.round(self.forward_prop(X)[0, 0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NN(layers=[\n",
    "\n",
    "    NN.Layer(neurons_amount=3, activation='sigmoid',),\n",
    "    NN.Layer(neurons_amount=1, activation='sigmoid',),\n",
    "\n",
    "], input_amount=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for epoch 1: 0.1659004836138328\n",
      "Cost for epoch 101: 0.12416569181717774\n",
      "Cost for epoch 201: 0.12292477734638053\n",
      "Cost for epoch 301: 0.12092037083412813\n",
      "Cost for epoch 401: 0.11737565785637757\n",
      "Cost for epoch 501: 0.11152585005756052\n",
      "Cost for epoch 601: 0.10306294371626813\n",
      "Cost for epoch 701: 0.09119904346181146\n",
      "Cost for epoch 801: 0.07319818461920005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for epoch 901: 0.050465515106219494\n",
      "Cost for epoch 1001: 0.03211677691825069\n",
      "Cost for epoch 1101: 0.021102827637749486\n",
      "Cost for epoch 1201: 0.014832872834363034\n",
      "Cost for epoch 1301: 0.01108352702267753\n",
      "Cost for epoch 1401: 0.008687531054968542\n",
      "Cost for epoch 1501: 0.007062371050368834\n",
      "Cost for epoch 1601: 0.005904577222270539\n",
      "Cost for epoch 1701: 0.005046258155491403\n",
      "Cost for epoch 1801: 0.004389013777959933\n",
      "Cost for epoch 1901: 0.00387216510180388\n",
      "Cost for epoch 2001: 0.003456621927618884\n",
      "Cost for epoch 2101: 0.0031162412929185753\n",
      "Cost for epoch 2201: 0.002832967948142407\n",
      "Cost for epoch 2301: 0.0025939811730818917\n",
      "Cost for epoch 2401: 0.002389954658887714\n",
      "Cost for epoch 2501: 0.0022139590651598516\n",
      "Cost for epoch 2601: 0.002060749084052529\n",
      "Cost for epoch 2701: 0.0019262879526781077\n",
      "Cost for epoch 2801: 0.0018074228050990902\n",
      "Cost for epoch 2901: 0.0017016583055282706\n",
      "Cost for epoch 3001: 0.0016069957954575188\n",
      "Cost for epoch 3101: 0.0015218170218619215\n",
      "Cost for epoch 3201: 0.0014447987748493335\n",
      "Cost for epoch 3301: 0.0013748493240280531\n",
      "Cost for epoch 3401: 0.001311060469597698\n",
      "Cost for epoch 3501: 0.0012526709394093153\n",
      "Cost for epoch 3601: 0.0011990381393241056\n",
      "Cost for epoch 3701: 0.0011496161286395528\n",
      "Cost for epoch 3801: 0.0011039382869756308\n",
      "Cost for epoch 3901: 0.0010616035538843962\n",
      "Cost for epoch 4001: 0.001022265415742343\n",
      "Cost for epoch 4101: 0.0009856230243932835\n",
      "Cost for epoch 4201: 0.0009514139839675712\n",
      "Cost for epoch 4301: 0.0009194084534950007\n",
      "Cost for epoch 4401: 0.0008894042951099722\n",
      "Cost for epoch 4501: 0.0008612230589629176\n",
      "Cost for epoch 4601: 0.000834706642106342\n",
      "Cost for epoch 4701: 0.0008097144936572365\n",
      "Cost for epoch 4801: 0.0007861212653401771\n",
      "Cost for epoch 4901: 0.0007638148271740549\n",
      "Cost for epoch 5001: 0.0007426945841014735\n",
      "Cost for epoch 5101: 0.0007226700418906892\n",
      "Cost for epoch 5201: 0.0007036595804948415\n",
      "Cost for epoch 5301: 0.0006855894008496794\n",
      "Cost for epoch 5401: 0.0006683926172951497\n",
      "Cost for epoch 5501: 0.000652008472769915\n",
      "Cost for epoch 5601: 0.0006363816579201869\n",
      "Cost for epoch 5701: 0.0006214617184910674\n",
      "Cost for epoch 5801: 0.0006072025379894555\n",
      "Cost for epoch 5901: 0.0005935618847455351\n",
      "Cost for epoch 6001: 0.0005805010142521404\n",
      "Cost for epoch 6101: 0.0005679843191027204\n",
      "Cost for epoch 6201: 0.0005559790200399027\n",
      "Cost for epoch 6301: 0.0005444548926143854\n",
      "Cost for epoch 6401: 0.0005333840247761592\n",
      "Cost for epoch 6501: 0.0005227406014070356\n",
      "Cost for epoch 6601: 0.000512500712379266\n",
      "Cost for epoch 6701: 0.0005026421812093425\n",
      "Cost for epoch 6801: 0.0004931444117845897\n",
      "Cost for epoch 6901: 0.00048398825098612704\n",
      "Cost for epoch 7001: 0.0004751558653251059\n",
      "Cost for epoch 7101: 0.00046663062995900976\n",
      "Cost for epoch 7201: 0.00045839702866821866\n",
      "Cost for epoch 7301: 0.0004504405635553103\n",
      "Cost for epoch 7401: 0.00044274767338642345\n",
      "Cost for epoch 7501: 0.0004353056596285485\n",
      "Cost for epoch 7601: 0.0004281026193528925\n",
      "Cost for epoch 7701: 0.0004211273842748453\n",
      "Cost for epoch 7801: 0.0004143694652879808\n",
      "Cost for epoch 7901: 0.00040781900192518014\n",
      "Cost for epoch 8001: 0.000401466716245538\n",
      "Cost for epoch 8101: 0.00039530387070301507\n",
      "Cost for epoch 8201: 0.0003893222296029241\n",
      "Cost for epoch 8301: 0.0003835140237960193\n",
      "Cost for epoch 8401: 0.0003778719182984298\n",
      "Cost for epoch 8501: 0.00037238898255942724\n",
      "Cost for epoch 8601: 0.00036705866312869244\n",
      "Cost for epoch 8701: 0.0003618747585009465\n",
      "Cost for epoch 8801: 0.00035683139593894423\n",
      "Cost for epoch 8901: 0.0003519230100962737\n",
      "Cost for epoch 9001: 0.00034714432327955446\n",
      "Cost for epoch 9101: 0.00034249032720569844\n",
      "Cost for epoch 9201: 0.00033795626612422784\n",
      "Cost for epoch 9301: 0.00033353762118730045\n",
      "Cost for epoch 9401: 0.0003292300959615577\n",
      "Cost for epoch 9501: 0.00032502960298594666\n",
      "Cost for epoch 9601: 0.0003209322512887382\n",
      "Cost for epoch 9701: 0.00031693433478511586\n",
      "Cost for epoch 9801: 0.0003130323214839099\n",
      "Cost for epoch 9901: 0.00030922284343865665\n"
     ]
    }
   ],
   "source": [
    "X_train_1 = np.array([[0, 0]])\n",
    "X_train_2 = np.array([[0, 1]])\n",
    "X_train_3 = np.array([[1, 0]])\n",
    "X_train_4 = np.array([[1, 1]])\n",
    "\n",
    "X_train = np.array([X_train_1, X_train_2, X_train_3, X_train_4 ])\n",
    "\n",
    "Y = np.array([\n",
    "    [[0]],\n",
    "    [[1]],\n",
    "    [[1]],\n",
    "    [[0]]\n",
    "    ])\n",
    "\n",
    "nn.train(X_train, Y, epochs=10000, glr=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR 00:  0.0\n",
      "XOR 01:  1.0\n",
      "XOR 10:  1.0\n",
      "XOR 11:  0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"XOR 00: \", nn.bin_predict(X_train_1))\n",
    "print(\"XOR 01: \", nn.bin_predict(X_train_2))\n",
    "print(\"XOR 10: \", nn.bin_predict(X_train_3))\n",
    "print(\"XOR 11: \", nn.bin_predict(X_train_4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
